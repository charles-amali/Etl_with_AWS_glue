{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Create SparkSession in Apache Spark"
      ],
      "metadata": {
        "id": "SXT8m_xCtjSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n"
      ],
      "metadata": {
        "id": "GU8eEI2FpwQm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"Spark-KPIs\").getOrCreate()\n"
      ],
      "metadata": {
        "id": "g9raSBbPqLJ5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "6an-zT7rsxBr",
        "outputId": "fc0fbd69-4e41-4d2f-cd0a-a469b758e35b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7e02c0231610>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://6743a0b04c43:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.5</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>Spark-KPIs</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading songs data\n"
      ],
      "metadata": {
        "id": "KPp6wEy_Jdb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "songs_file_path = \"/content/songs.csv\"\n",
        "songs_df = spark.read.csv(songs_file_path, header=True, inferSchema=True)\n",
        "songs_df.printSchema()\n",
        "songs_df.show(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "Qx7BhOmUuIvz",
        "outputId": "62cfe2ce-5609-45f8-cc6c-b51f9d0c5625"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/content/songs.csv.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-a11aeba05e41>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0msongs_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/songs.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msongs_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msongs_file_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0msongs_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msongs_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/content/songs.csv."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Cleaning\n"
      ],
      "metadata": {
        "id": "76jLoNopDC1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "songs_df.count()"
      ],
      "metadata": {
        "id": "zhaj19Oc9uwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "songs_df.describe().show()"
      ],
      "metadata": {
        "id": "sdu7hGC8kbvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check for missing values\n",
        "from pyspark.sql.functions import col, isnan, sum, count\n",
        "\n",
        "songs_df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in songs_df.columns]).show()\n",
        "# songs_df.agg(*[count(col(c).isNull().cast(\"int\")).alias(c) for c in songs_df.columns]).show()\n",
        "# songs_df.select([col(c).isNull().alias(c) for c in songs_df.columns]).show()\n"
      ],
      "metadata": {
        "id": "cbteOKsBC4cB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing rows with null values\n",
        "songs_df = songs_df.dropna()\n",
        "songs_df.count()"
      ],
      "metadata": {
        "id": "ae1dUqKMDYYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing Duplicate rows\n",
        "songs_df_unique = songs_df.dropDuplicates()\n",
        "songs_df_unique.count()\n"
      ],
      "metadata": {
        "id": "y22d5K8xH62o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_casted = df.withColumn(\"Age\", df[\"Age\"].cast(\"Integer\"))\n",
        "# df_casted.printSchema()"
      ],
      "metadata": {
        "id": "j96nlJ30Lrsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading user data"
      ],
      "metadata": {
        "id": "HgkazFTuJliL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "users_file_path = \"/content/users.csv\"\n",
        "users_df = spark.read.csv(users_file_path, header=True, inferSchema=True)\n",
        "users_df.printSchema()\n",
        "users_df.show(20)"
      ],
      "metadata": {
        "id": "2voCP2-uIY85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users_df.count()"
      ],
      "metadata": {
        "id": "xiiggKEkKCwU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "users_df.describe().show()"
      ],
      "metadata": {
        "id": "zmrsxMqolByq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check for missing values\n",
        "from pyspark.sql.functions import col, isnan, sum\n",
        "\n",
        "users_df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in users_df.columns]).show()"
      ],
      "metadata": {
        "id": "b-KJYagUKSFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Removing Duplicate rows\n",
        "users_df_unique = users_df.dropDuplicates()\n",
        "users_df_unique.count()"
      ],
      "metadata": {
        "id": "Nve9w9I5KjQL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading stream data\n"
      ],
      "metadata": {
        "id": "PM7HRjfUhUSr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "streams1 = spark.read.csv(\"/content/streams1.csv\", header=True, inferSchema=True)\n",
        "streams2 = spark.read.csv(\"/content/streams2.csv\", header=True, inferSchema=True)\n",
        "streams3 = spark.read.csv(\"/content/streams3.csv\", header=True, inferSchema=True)\n",
        "streams3.printSchema()"
      ],
      "metadata": {
        "id": "U3csXCcjLLL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#concat the streams data\n",
        "streams = streams1.union(streams2).union(streams3)\n",
        "streams.show(20)"
      ],
      "metadata": {
        "id": "mSXhy9mJimWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# rename the first track_id column to avoid ambiguity\n",
        "streams = streams.withColumnRenamed(\"track_id\", \"track_id_1\")\n"
      ],
      "metadata": {
        "id": "j_8n-eJSjcU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "streams.describe().show()"
      ],
      "metadata": {
        "id": "cydheXiwj1lY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Joining song data with streams and user data"
      ],
      "metadata": {
        "id": "e6pHvxfSnavB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "song_user_streams = songs_df.join(streams, songs_df.track_id == streams.track_id_1, 'inner') \\\n",
        "                  .join(users_df, streams.user_id == users_df.user_id, 'inner') \\\n",
        "                  .select(songs_df['*'],\n",
        "                         streams.user_id.alias('stream_user_id'),\n",
        "                         streams.track_id_1,\n",
        "                         streams.listen_time,\n",
        "                         users_df.user_id.alias('user_user_id'),\n",
        "                         users_df.user_name,\n",
        "                         users_df.user_age,\n",
        "                         users_df.user_country,\n",
        "                         users_df.created_at)\n",
        "\n",
        "song_user_streams.show()"
      ],
      "metadata": {
        "id": "2Gi_Kba3kCiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "song_user_streams.describe().show()"
      ],
      "metadata": {
        "id": "GjuNGLDjnOnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "song_user_streams.columns"
      ],
      "metadata": {
        "id": "UsuIy_kCntfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Computing KPIs"
      ],
      "metadata": {
        "id": "Q7HGs0z1mu_I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import to_date, count, countDistinct, desc, rank"
      ],
      "metadata": {
        "id": "8K81E-d1ocds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "listen_count = song_user_streams.groupBy(\"track_genre\", to_date(\"created_at\").alias(\"date\")) \\\n",
        "                 .agg(count(\"track_id\").alias(\"listen_count\"))\n",
        "listen_count.show()"
      ],
      "metadata": {
        "id": "Ls2E7Fximf7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_listeners = song_user_streams.groupBy(\"track_genre\", to_date(\"created_at\").alias(\"date\")) \\\n",
        "                     .agg(countDistinct(\"user_user_id\").alias(\"unique_listeners\"))\n",
        "unique_listeners.show()"
      ],
      "metadata": {
        "id": "SaN6fjhfnLOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_listening_time = song_user_streams.groupBy(\"track_genre\", to_date(\"created_at\").alias(\"date\")) \\\n",
        "                         .agg(sum(\"duration_ms\").alias(\"total_listening_time\"))\n",
        "total_listening_time.show()"
      ],
      "metadata": {
        "id": "Ed-04K45p2i0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "avg_listening_time_per_user = total_listening_time.join(unique_listeners, [\"track_genre\", \"date\"]) \\\n",
        "                                                  .withColumn(\"avg_listening_time_per_user\",\n",
        "                                                              col(\"total_listening_time\") / col(\"unique_listeners\"))\n",
        "avg_listening_time_per_user.show()"
      ],
      "metadata": {
        "id": "wLPlueshv1DX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Compute listen count per song per genre per day\n",
        "song_listen_count = song_user_streams.groupBy(\n",
        "    to_date(\"created_at\").alias(\"date\"),\n",
        "    \"track_genre\",\n",
        "    \"track_name\"\n",
        ").agg(countDistinct(\"track_id\").alias(\"listen_count\"))\n",
        "\n",
        "\n",
        "song_rank_window = Window.partitionBy(\"date\", \"track_genre\").orderBy(desc(\"listen_count\"))\n",
        "\n",
        "top_songs_per_genre = song_listen_count.withColumn(\"rank\", rank().over(song_rank_window)) \\\n",
        "                                     .filter(col(\"rank\") <= 3)\n",
        "\n",
        "top_songs_per_genre.show()"
      ],
      "metadata": {
        "id": "Z8Sf1bhuv4cS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# top_songs_per_genre = song_user_streams.groupBy(\n",
        "#     to_date(\"created_at\").alias(\"date\"),\n",
        "#     \"track_name\",\n",
        "#     \"track_genre\"\n",
        "# ).agg(countDistinct(\"track_id\").alias(\"listen_count\"))\\\n",
        "#  .withColumn(\"rank\", rank().over(Window.partitionBy(\"date\").orderBy(desc(\"listen_count\"))))\\\n",
        "#         .filter(col(\"rank\") <= 3)\n",
        "\n",
        "# top_songs_per_genre.show()"
      ],
      "metadata": {
        "id": "StEF5XCGEpxH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "song_listen_count = song_user_streams.groupBy(\n",
        "    to_date(\"created_at\").alias(\"date\"), \"track_genre\", \"track_name\"\n",
        ").agg(\n",
        "    countDistinct(\"track_id\").alias(\"listen_count\")  # Count the number of plays per song\n",
        ")\n",
        "\n",
        "# Step 2: Define ranking window partitioned by date & genre, ordered by listen count\n",
        "rank_window = Window.partitionBy(\"date\", \"track_genre\").orderBy(desc(\"listen_count\"))\n",
        "\n",
        "# Step 3: Rank songs and filter for the top 3 per genre per day\n",
        "top_songs_per_genre = song_listen_count.withColumn(\"rank\", rank().over(rank_window)) \\\n",
        "                                       .filter(col(\"rank\") <= 3)\n",
        "\n",
        "# Step 4: Show results\n",
        "top_songs_per_genre.show()"
      ],
      "metadata": {
        "id": "haEhI_0oM6xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "\n",
        "# Step 1: Compute listen count per genre per day\n",
        "genre_listen_count = song_user_streams.groupBy(\n",
        "    F.to_date(\"created_at\").alias(\"date\"),\n",
        "    \"track_genre\"\n",
        ").agg(F.count(\"track_id\").alias(\"genre_listen_count\"))\n",
        "\n",
        "# Step 2: Define a ranking window for top genres per day\n",
        "genre_rank_window = Window.partitionBy(\"date\").orderBy(F.desc(\"genre_listen_count\"))\n",
        "\n",
        "# Step 3: Apply ranking and filter for the top 5 genres per day\n",
        "top_genres_per_day = genre_listen_count.withColumn(\"5_gen_rank\", F.rank().over(genre_rank_window)) \\\n",
        "                                       .filter(F.col(\"5_gen_rank\") <= 5)\n",
        "\n",
        "top_genres_per_day.show()"
      ],
      "metadata": {
        "id": "-5QkaJflGf9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9V2jFj-IPTy6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}